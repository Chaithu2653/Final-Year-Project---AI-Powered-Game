## Overview ğŸ®ğŸ§ âœ¨
Interactive LLM-Powered NPCs is an open-source project that redefines how players interact with non-player characters (NPCs) in games. With this system, you can speak directly to NPCs using your microphone and engage in dynamic, unscripted conversationsâ€”bringing a whole new level of realism to your favorite titles.

Powered by technologies like SadTalker for real-time lip-syncing, facial recognition to distinguish NPCs, and vector stores for persistent memory, the system tailors each interaction to match the characterâ€™s personality and knowledge. It even detects your facial expressions through your webcam ğŸ¥, adding emotional depth to every encounter.

Built for existing games like Cyberpunk 2077, Assassinâ€™s Creed, and GTA V, this project enhances worlds that already feel aliveâ€”but lacked true conversational immersion. Now, players can talk to any NPC and experience story-rich gameplay that the original developers never included.

No need to dive into game files or do heavy modding ğŸ› ï¸. Instead, the system cleverly replaces facial pixels in real time, seamlessly blending animated expressions into the game environment.

Whether you're climbing ancient towers in Assassinâ€™s Creed Valhalla or strolling through the neon-lit streets of Night City, Interactive LLM-Powered NPCs delivers immersive, lifelike conversations that bring the virtual world to life like never before ğŸš€.

## How It Works ğŸ§©ğŸ—£ï¸
The magic behind Interactive LLM-Powered NPCs is just as impressive as the conversations it creates. Here's how everything comes together to deliver lifelike interactions in your favorite games:

ğŸ¤ Voice Input
Simply speak into your microphoneâ€”your voice is instantly converted into text, becoming the starting point for the interaction.

ğŸ§â€â™‚ï¸ Facial Recognition
The system uses facial recognition to detect which NPC you're talking to, whether it's a major side character or a random background figure.

ğŸ” Character Identification
Once recognized, a unique personality and name are created for background NPCs. For known side characters, the system accesses their pre-defined traits, backstory, and knowledge.

ğŸ§  LLM Response Generation
The transcribed dialogue and character details are passed to a Large Language Model (LLM). It uses this informationâ€”along with a vector store containing lore, memory, and character-specific contextâ€”to generate an appropriate response.

ğŸ“„ Pre-Conversation Prompts
To ensure authenticity, the system references a pre-conversation.json file, which includes signature lines and speech patterns for each NPC. These help the LLM maintain the character's voice and personality.

ğŸ—£ï¸ Speech and Lip Sync
The LLM's response is transformed into speech using text-to-speech synthesis. A facial animation is then generated by applying this voice to the NPCâ€™s face, producing realistic lip movements using tools like SadTalker.

ğŸ® In-Game Integration
This facial animation video is layered onto the game by replacing the NPCâ€™s on-screen facial pixels. It looks like the character is genuinely talkingâ€”without changing the gameâ€™s source code or assets.

ğŸ˜Š Emotion Detection
Your webcam captures your facial expressions in real-time. NPCs can detect your mood and adjust their tone and responses to match, creating deeper, emotionally aware conversations.

ğŸ‡ Hands-Free, Anywhere Interactions
The system supports conversations even when the NPCâ€™s face isnâ€™t visibleâ€”like during horseback rides, distant scenes, or combat. Just say the character's name, followed by your message, and the interaction unfolds naturallyâ€”dialogue, response, and voice included.

ğŸ•¹ï¸ Game-Friendly Design
No mods or source code edits are required. This system is designed to work with any game, seamlessly overlaying itself on top of existing visuals.

## Prerequisites ğŸš€ğŸ”§
ğŸ Python 3.10.6
Download and install from: https://www.python.org/downloads/release/python-3106/
âœ… Make sure to check the "Add to PATH" option during installation.

ğŸ™ GIT
Download and install Git from: https://git-scm.com/downloads

ğŸŒ wget
Install wget, a command-line tool to download files from the web.

ğŸ› ï¸ Microsoft Build Tools and Visual Studio
Install from the following links:
ğŸ”— https://visualstudio.microsoft.com/downloads/?q=build+tools#visual-studio-professional-2022
ğŸ”— https://visualstudio.microsoft.com/downloads/?q=build+tools#build-tools-for-visual-studio-2022

ğŸ¥ğŸ”Š FFmpeg
Install FFmpeg for audio and video processing.
ğŸ“˜ Instructions: https://www.wikihow.com/Install-FFmpeg-on-Windows

Installation ğŸ”Œâœ¨
Open a terminal.

Clone the repository:
git clone https://github.com/AkshitIreddy/Interactive-LLM-Powered-NPCs.git

Navigate to the repository folder:
cd Interactive-LLM-Powered-NPCs

Create a virtual environment:
python -m venv .venv

Activate the environment:
.venv\scripts\activate

Install dependencies:
pip install -r requirements.txt

Open Git Bash and navigate to the project folder.

Go to SadTalker directory:
cd sadtalker

Download models:
bash scripts/download_models.sh

Double-click "webui.bat" inside "sadtalker" and wait until you see â€œWebUI launchedâ€ then close the terminal.

ğŸ†“ Create a Cohere account and add your trial API key to apikeys.json

ğŸ—‘ï¸ Delete all files and folders inside the video_temp and temp folders.

VSCode Setup with Jupyter Support ğŸ‘¨â€ğŸ’»ğŸ“”
Install Visual Studio Code.

Click Extensions icon (ğŸ§©) and install:

Jupyter ğŸ““

Python ğŸ

Open terminal â†’ navigate to root folder â†’ type:
code .

If Jupyter doesn't detect .venv as a kernel, go to File > Open Folder and reselect the root project folder.

Directions for Use ğŸ› ï¸ğŸ¤“
ğŸ“ Create a folder in root with your game name, replacing spaces with underscores.
Example: Assasins_Creed_Valhalla

ğŸ“ Inside it, create world.txt â€” describe your game's world.

ğŸ“ Create public_info.txt â€” details any lore, major events, known facts.

ğŸ“” Open create_public_vectordb.ipynb, set game_name, run both cells to create and test public vector DB.

ğŸ“‚ Inside your game folder, create npc_personality_creation
â• Copy audio_mode_create_personality.py and video_mode_personality.py from Cyberpunk_2077/npc_personality_creation
ğŸ§  Modify the template variable with NPC names and personalities relevant to your game.
ğŸ’¡ Use ChatGPT to help generate new ones.
â• Replace the same files inside the functions folder.

ğŸ“‚ Inside game folder, create a folder called characters
â• Copy the default folder from Cyberpunk_2077/characters
âœï¸ Edit pre_conversation.json with common NPC lines from your game.
ğŸ’¬ Use prompt:
â€œGive 30 common dialogue that people in [Your Game] say, in this format:
{ "pre_conversation": [ { "line": "..." }, ... ] }â€

ğŸ“‚ For each side character, create a folder inside characters
ğŸ“¸ Inside it, add an images folder with 5 photos of the character (JPG only, no other people).

ğŸ“” Open create_face_recognition_representation.ipynb
â• Set character_name and game_name, run both cells to generate face data.

ğŸ“ Inside character folder, create bio.txt â€” a short summary of the character.

ğŸ“ Create character_knowledge.txt â€” private info not in public DB.

ğŸ“” Open create_character_vectordb.ipynb, set the variables, and run to generate their vector DB.

ğŸ“ Create pre_conversation.json â€” with lines the character commonly says.

ğŸ“ Create conversation.json â€” copy from another character like Jackie Welles and modify.

ğŸ“‚ Create a voice folder
â• Add voice.py with a create_speech function
ğŸ™ï¸ Modify it to match your characterâ€™s voice
ğŸ§ª Use voice_selection.ipynb to pick a voice
ğŸ—£ï¸ If using voice cloning, keep the same create_speech signature and avoid relative paths.

ğŸ” Repeat for more characters.

Play the Game ğŸ®ğŸš€
Launch your game.

Open main.ipynb

Set: player_name, game_name, interact_key

Run the second cell

If using two monitors: drag window to second screen

If single monitor: copy contents from miscellaneous/Single Monitor to root

Set game_screen_name in main.ipynb

Point camera at an NPC
â¡ï¸ Click the new window
â¡ï¸ Hold the interact key
ğŸ¤ Speak your dialogue
ğŸ—£ï¸ Youâ€™ll see a response with voice + facial animation.

ğŸ”Š Can interact with offscreen NPCs (horse rides, combat, etc.).

## Conclusion and Contribution ğŸ¤ğŸ®
Interactive LLM Powered NPCs adds deep realism to your games through natural conversations.
ğŸ’¡ Add your game to the Games folder and submit a Pull Request to help others.
ğŸ‘¨â€ğŸ’» You can also contribute code, fix bugs, or suggest ideas.
Letâ€™s reshape how gamers interact with NPCs, together.

## Tools Used ğŸš€ğŸ§°
ğŸª Cohere + LangChain for LLMs

ğŸ© SadTalker for facial animation

ğŸ° Edge-TTS for default voices

ğŸ§ DeepFace for facial recognition and emotion detection

ğŸ­ Chromadb for vector DBs

ğŸ¬ SpeechRecognition for voice-to-text
